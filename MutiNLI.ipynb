{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://github.com/kohpangwei/group_DRO/blob/master/data/multinli_dataset.py\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from models import model_attributes\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from data.confounder_dataset import ConfounderDataset\n",
    "\n",
    "class MultiNLIDataset(ConfounderDataset):\n",
    "    \"\"\"\n",
    "    MultiNLI dataset.\n",
    "    label_dict = {\n",
    "        'contradiction': 0,\n",
    "        'entailment': 1,\n",
    "        'neutral': 2\n",
    "    }\n",
    "    # Negation words taken from https://arxiv.org/pdf/1803.02324.pdf\n",
    "    negation_words = ['nobody', 'no', 'never', 'nothing']\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir,\n",
    "                 target_name, confounder_names,\n",
    "                 augment_data=False,\n",
    "                 model_type=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.target_name = target_name\n",
    "        self.confounder_names = confounder_names\n",
    "        self.model_type = model_type\n",
    "        self.augment_data = augment_data\n",
    "\n",
    "        assert len(confounder_names) == 1\n",
    "        assert confounder_names[0] == 'sentence2_has_negation'\n",
    "        assert target_name in ['gold_label_preset', 'gold_label_random']\n",
    "        assert augment_data == False\n",
    "        assert model_type == 'bert'\n",
    "\n",
    "        self.data_dir = os.path.join(\n",
    "            self.root_dir,\n",
    "            'data')\n",
    "        self.glue_dir = os.path.join(\n",
    "            self.root_dir,\n",
    "            'glue_data',\n",
    "            'MNLI')\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            raise ValueError(\n",
    "                f'{self.data_dir} does not exist yet. Please generate the dataset first.')\n",
    "        if not os.path.exists(self.glue_dir):\n",
    "            raise ValueError(\n",
    "                f'{self.glue_dir} does not exist yet. Please generate the dataset first.')\n",
    "\n",
    "        # Read in metadata\n",
    "        type_of_split = target_name.split('_')[-1]\n",
    "        self.metadata_df = pd.read_csv(\n",
    "            os.path.join(\n",
    "                self.data_dir,\n",
    "                f'metadata_{type_of_split}.csv'),\n",
    "            index_col=0)\n",
    "\n",
    "        # Get the y values\n",
    "        # gold_label is hardcoded\n",
    "        self.y_array = self.metadata_df['gold_label'].values\n",
    "        self.n_classes = len(np.unique(self.y_array))\n",
    "\n",
    "        self.confounder_array = self.metadata_df[confounder_names[0]].values\n",
    "        self.n_confounders = len(confounder_names)\n",
    "\n",
    "        # Map to groups\n",
    "        self.n_groups = len(np.unique(self.confounder_array)) * self.n_classes\n",
    "        self.group_array = (self.y_array*(self.n_groups/self.n_classes) + self.confounder_array).astype('int')\n",
    "\n",
    "        # Extract splits\n",
    "        self.split_array = self.metadata_df['split'].values\n",
    "        self.split_dict = {\n",
    "            'train': 0,\n",
    "            'val': 1,\n",
    "            'test': 2\n",
    "        }\n",
    "\n",
    "        # Load features\n",
    "        self.features_array = []\n",
    "        for feature_file in [\n",
    "            'cached_train_bert-base-uncased_128_mnli',  \n",
    "            'cached_dev_bert-base-uncased_128_mnli',\n",
    "            'cached_dev_bert-base-uncased_128_mnli-mm'\n",
    "            ]:\n",
    "\n",
    "            features = torch.load(\n",
    "                os.path.join(\n",
    "                    self.glue_dir,\n",
    "                    feature_file))\n",
    "\n",
    "            self.features_array += features\n",
    "\n",
    "        self.all_input_ids = torch.tensor([f.input_ids for f in self.features_array], dtype=torch.long)\n",
    "        self.all_input_masks = torch.tensor([f.input_mask for f in self.features_array], dtype=torch.long)\n",
    "        self.all_segment_ids = torch.tensor([f.segment_ids for f in self.features_array], dtype=torch.long)\n",
    "        self.all_label_ids = torch.tensor([f.label_id for f in self.features_array], dtype=torch.long)\n",
    "\n",
    "        self.x_array = torch.stack((\n",
    "            self.all_input_ids,\n",
    "            self.all_input_masks,\n",
    "            self.all_segment_ids), dim=2)\n",
    "\n",
    "        assert np.all(np.array(self.all_label_ids) == self.y_array)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_array)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        y = self.y_array[idx]\n",
    "        g = self.group_array[idx]\n",
    "        x = self.x_array[idx, ...]\n",
    "        return x, y, g\n",
    "\n",
    "    def group_str(self, group_idx):\n",
    "        y = group_idx // (self.n_groups/self.n_classes)\n",
    "        c = group_idx % (self.n_groups//self.n_classes)\n",
    "\n",
    "        attr_name = self.confounder_names[0]\n",
    "        group_name = f'{self.target_name} = {int(y)}, {attr_name} = {int(c)}'\n",
    "        return group_name"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
