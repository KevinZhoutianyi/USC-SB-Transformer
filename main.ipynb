{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=2, epochs=50, gamma=1, lr=0.01, max_length=256, model_name='roberta-base', num_workers=0, train_num_points=500, valid_num_points=100)\n",
      "premise {'input_ids': tensor([[    0,  9157, 16771,  ...,     1,     1,     1],\n",
      "        [    0,  6968,   216,  ...,     1,     1,     1],\n",
      "        [    0,  3762,     9,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,   133,  8725,  ...,     1,     1,     1],\n",
      "        [    0,   170,  2885,  ...,     1,     1,     1],\n",
      "        [    0, 33049,   957,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "hyper {'input_ids': tensor([[    0, 41257,     8,  ...,     1,     1,     1],\n",
      "        [    0,  1185,  2217,  ...,     1,     1,     1],\n",
      "        [    0,   250,   919,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,   487,   281,  ...,     1,     1,     1],\n",
      "        [    0, 43560,   254,  ...,     1,     1,     1],\n",
      "        [    0, 33049,   957,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "input tensor([    0,  9157, 16771, 13851,  6353,  2972, 16364,    34,    80,  3280,\n",
      "        22735,   111,  1152,     8, 18947,     4,     2,     2, 41257,     8,\n",
      "        18947,    32,    99,   146,  6353,  2972, 16364,   173,     4,  1437,\n",
      "            2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1])\n",
      "att tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "premise {'input_ids': tensor([[    0, 35716,     6,  ...,     1,     1,     1],\n",
      "        [    0,   783, 11380,  ...,     1,     1,     1],\n",
      "        [    0,   133,  5253,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0, 18691, 34015,  ...,     1,     1,     1],\n",
      "        [    0,   673,  5881,  ...,     1,     1,     1],\n",
      "        [    0,   179,    14,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "hyper {'input_ids': tensor([[    0, 29375,   782,  ...,     1,     1,     1],\n",
      "        [    0,  1711,    16,  ...,     1,     1,     1],\n",
      "        [    0,   133,  5253,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,   133,  5589,  ...,     1,     1,     1],\n",
      "        [    0,   133,  6168,  ...,     1,     1,     1],\n",
      "        [    0,  2387,  1623,  ...,     2,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 0, 0]])}\n",
      "input tensor([    0, 35716,     6,    51,    74,   240,     7,   216,     5,  2167,\n",
      "         6494,  8480,   416,     4,     2,     2, 29375,   782,     7,   216,\n",
      "         2167,  3748,  8480,     2,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1])\n",
      "att tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "os.getcwd() \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "from test import *\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "import torch\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from RoBERTa import *\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# %%\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "parser = argparse.ArgumentParser(\"main\")\n",
    "\n",
    "parser.add_argument('--valid_num_points', type=int,             default = 100,              help='validation data number')\n",
    "parser.add_argument('--train_num_points', type=int,             default = 500,              help='train data number')\n",
    "parser.add_argument('--model_name',       type=str,             default = 'roberta-base',   help='model name')\n",
    "parser.add_argument('--max_length',       type=int,             default=256,                help='max_length')\n",
    "parser.add_argument('--batch_size',       type=int,             default=2,                  help='Batch size')\n",
    "parser.add_argument('--num_workers',      type=int,             default=0,                  help='num_workers')\n",
    "parser.add_argument('--epochs',           type=int,             default=50,                  help='num of epochs')\n",
    "parser.add_argument('--lr',               type=float,           default=1e-2,               help='lr')\n",
    "parser.add_argument('--gamma',            type=float,           default=1,                  help='lr*gamma after each test')\n",
    "\n",
    "args = parser.parse_args(args=[])#(args=['--batch_size', '8',  '--no_cuda'])#used in ipynb\n",
    "print(args)\n",
    "\n",
    "dataset = load_dataset('glue', 'mnli')\n",
    "\n",
    "# %%\n",
    "train = dataset['train'][:args.train_num_points]\n",
    "valid = dataset['train'][-args.valid_num_points:]\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(args.model_name)\n",
    "#mnli\n",
    "#The Multi-Genre Natural Language Inference Corpus is a crowdsourced collection of sentence pairs with textual entailment annotations. Given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment), contradicts the hypothesis (contradiction), or neither (neutral). The premise sentences are gathered from ten different sources, including transcribed speech, fiction, and government reports. The authors of the benchmark use the standard test set, for which they obtained private labels from the RTE authors, and evaluate on both the matched (in-domain) and mismatched (cross-domain) section. They also uses and recommend the SNLI corpus as 550k examples of auxiliary training data.\n",
    "\n",
    "# %%\n",
    "train_data = get_Dataset(train, tokenizer,max_length=args.max_length)\n",
    "train_dataloader = DataLoader(train_data, sampler= SequentialSampler(train_data), \n",
    "                        batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)\n",
    "valid_data = get_Dataset(valid, tokenizer,max_length=args.max_length)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=SequentialSampler(valid_data), \n",
    "                        batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)\n",
    "\n",
    "# %%\n",
    "# model = TextClassifier(args).to(device)\n",
    "# model.train(train_dataloader,valid_dataloader,device)\n",
    "    \n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoding = tokenizer(text=['hi im kevin','hi hi hihi hi hihi hi hi '],text_pair =['whassup','yes yes ys'], return_tensors='pt', add_special_tokens=True, padding=True, truncation = True, max_length = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  3592,  4356,  7321,  6320,     2,     2, 11613,  2401,   658,\n",
       "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "        [    0,  3592, 20280,  1368,  4001,   118, 20280,  1368,  4001,   118,\n",
       "         20280, 20280,  1437,     2,     2, 10932,  4420,  1423,    29,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([    0, 35716,     6,    51,    74,   240,     7,   216,     5,  2167,\n",
    "         6494,  8480,   416,     4,     2,     2, 29375,   782,     7,   216,\n",
    "         2167,  3748,  8480,     2,     1,     1,     1,     1,     1,     1,\n",
    "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "            1,     1,     1,     1,     1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
