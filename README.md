# USC-SB-Transformer


1. download [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta) from huggingface(HF)
  
   Pretrained [RoBERTa-base](https://huggingface.co/roberta-base)

   Trained on MultiNLP [RoBERTa-large](https://huggingface.co/roberta-large-mnli)


2. download [MultiNLI](https://github.com/kohpangwei/group_DRO/blob/master/data/multinli_dataset.py)

3. Implement the RoBERTa on MultiNLI:

TODO: Train model on Server