{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from utils import accuracy_score\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_torch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 9, 768])\n",
      "torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "text_batch = [\"I love Pixar.\", \"I don't care for Pixar.\"]\n",
    "encoding = tokenizer(text_batch, return_tensors='pt', padding=True, truncation=True)\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']\n",
    "labels = torch.tensor([1,0]).unsqueeze(0)\n",
    "outputs = model(input_ids, attention_mask=attention_mask)\n",
    "print(outputs.last_hidden_state.shape) #number of data, sequence length, hidden state size\n",
    "print(outputs.last_hidden_state[:,0,:].shape) # connect this to FC layer (768,2) to do binary classfication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, args, model_name='roberta-base', num_groups=3, batch_size=32, max_length=128):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        self.model = RobertaModel.from_pretrained(model_name)\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.fc = nn.Linear(self.model.config.hidden_size, 3)  # FC layer\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()#ignore_index=0\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(),  lr= args.lr ,  betas=(0, 0.9)  )\n",
    "        self.scheduler =torch.optim.lr_scheduler.StepLR(self.optimizer, 1, gamma=args.gamma)\n",
    "        self.epoch = args.epochs\n",
    "\n",
    "    def forward(self,x,x_att):\n",
    "        output = self.model(x,attention_mask=x_att)\n",
    "        hidden_states = output.last_hidden_state[:,0,:] #[cls] tokens\n",
    "        logits = self.fc(hidden_states)\n",
    "        logits = self.softmax(logits) #not sure whether we need this line to compute loss\n",
    "        return logits\n",
    "    \n",
    "    def loss(self,logits,labels):\n",
    "        loss = self.criterion(logits, labels)\n",
    "        return loss\n",
    "\n",
    "    def train(self,train_dataloader,valid_dataloader,device):\n",
    "        model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss = 0.0\n",
    "            for step,batch in enumerate(train_dataloader):\n",
    "                input_ids = Variable(batch[0], requires_grad=False).to(device, non_blocking=False)\n",
    "                input_attn = Variable(batch[1], requires_grad=False).to(device, non_blocking=False)\n",
    "                labels = Variable(batch[2], requires_grad=False).to(device, non_blocking=False)    \n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                logits = self.forward(input_ids,input_attn)\n",
    "                loss = self.loss(logits,labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{self.epochs}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for step,batch in enumerate(train_dataloader):\n",
    "                    input_ids = Variable(batch[0], requires_grad=False).to(device, non_blocking=False)\n",
    "                    input_attn = Variable(batch[1], requires_grad=False).to(device, non_blocking=False)\n",
    "                    labels = Variable(batch[2], requires_grad=False).to(device, non_blocking=False)   \n",
    "                    \n",
    "                    logits = self.forward(input_ids,input_attn)\n",
    "                    loss = self.loss(logits,labels)\n",
    "\n",
    "                    predict = torch.argmax(logits,dim=1)\n",
    "                    accuracy = accuracy_score(labels, predict)\n",
    "\n",
    "\n",
    "            print(f\"Validation Loss: {loss:.4f}\")\n",
    "            print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "            self.model.train()\n",
    "\n",
    "   \n",
    "\n",
    "# classifier = RoBERTaTextClassifier(\"roberta-base\", num_labels=3)\n",
    "# train_texts, train_labels, validation_texts, validation_labels = ... multiNLP\n",
    "# classifier.train(train_texts, train_labels, validation_texts, validation_labels)\n",
    "# test_texts = ...\n",
    "# predictions = classifier.predict(test_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
