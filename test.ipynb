{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/youqihuang/Desktop/USC-SB-Transformer/.env/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(train_num_points=1000, valid_num_points=100, report_num_points=500, model_name='roberta-base', max_length=128, replace_size=3, batch_size=2, num_workers=0, epochs=1, lr=1e-05, gamma=1)\n",
      "\n",
      " Property of dataset:\n",
      "train set size:  392702\n",
      "validation_matched set size:  9815\n",
      "validation_mismatched set size:  9832\n",
      "test_matched set size:  9796\n",
      "test_mismatched set size:  9847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor id shape torch.Size([100, 128, 128])\n",
      "tensor attention shape torch.Size([100, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2, 128, 128])\n",
      "torch.Size([2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [36, 128] at entry 0 and [30, 128] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 87\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[1;32m     86\u001b[0m model \u001b[38;5;241m=\u001b[39m TextClassifier(args)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 87\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreplaced_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/USC-SB-Transformer/RoBERTa.py:111\u001b[0m, in \u001b[0;36mTextClassifier.train\u001b[0;34m(self, train_dataloader, valid_dataloader, replaced_dataloader, device)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m,train_dataloader,valid_dataloader,replaced_dataloader, device):\n\u001b[1;32m    110\u001b[0m     logger \u001b[38;5;241m=\u001b[39m  logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreplaced_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    113\u001b[0m     report_counter  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/USC-SB-Transformer/RoBERTa.py:94\u001b[0m, in \u001b[0;36mTextClassifier.validation\u001b[0;34m(self, valid_dataloader, replaced_dataloader, epoch, device, save)\u001b[0m\n\u001b[1;32m     92\u001b[0m         all_loss\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     93\u001b[0m         all_acc\u001b[38;5;241m.\u001b[39mappend(accuracy)\n\u001b[0;32m---> 94\u001b[0m sensitivity \u001b[38;5;241m=\u001b[39m \u001b[43mword_label_sensitivity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplaced_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msensitivity_2dlist_report\u001b[38;5;241m.\u001b[39mappend(sensitivity)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_loss_report\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28msum\u001b[39m(all_loss)    \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_loss) )\n",
      "File \u001b[0;32m~/Desktop/USC-SB-Transformer/utils.py:189\u001b[0m, in \u001b[0;36mword_label_sensitivity\u001b[0;34m(replaced_dataloader, original_dataloader, model, device, n)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# Filter out tensors that contain only zeros in padded_id\u001b[39;00m\n\u001b[1;32m    188\u001b[0m rep_input_ids \u001b[38;5;241m=\u001b[39m [[tensor \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m row \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m contains_only_zeros(tensor)] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m replaced_batch[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 189\u001b[0m rep_input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrep_input_ids\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Filter out tensors that contain only zeros in padded_attn\u001b[39;00m\n\u001b[1;32m    192\u001b[0m rep_att_masks \u001b[38;5;241m=\u001b[39m [[tensor \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m row \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m contains_only_zeros(tensor)] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m replaced_batch[\u001b[38;5;241m1\u001b[39m]]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [36, 128] at entry 0 and [30, 128] at entry 1"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# %%\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "os.getcwd() \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "from test import *\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "import torch\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from RoBERTa import *\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "parser = argparse.ArgumentParser(\"main\")\n",
    "\n",
    "parser.add_argument('--train_num_points', type=int,             default = 1000,              help='train data number')\n",
    "parser.add_argument('--valid_num_points', type=int,             default = 100,              help='validation data number')\n",
    "parser.add_argument('--report_num_points',type=int,             default = 500,              help='report number')\n",
    "parser.add_argument('--model_name',       type=str,             default = 'roberta-base',   help='model name')\n",
    "parser.add_argument('--max_length',       type=int,             default=128,                help='max_length')\n",
    "parser.add_argument('--replace_size',     type=int,             default=3,                  help='to test sensitivity, we need to replance each word by x random words from vocab, here we specify the x')\n",
    "#TODO: default = 64\n",
    "parser.add_argument('--batch_size',       type=int,             default=2,                  help='Batch size')\n",
    "parser.add_argument('--num_workers',      type=int,             default=0,                  help='num_workers')\n",
    "parser.add_argument('--epochs',           type=int,             default=1,                  help='num of epochs')\n",
    "parser.add_argument('--lr',               type=float,           default=1e-5,               help='lr')\n",
    "parser.add_argument('--gamma',            type=float,           default=1,                  help='lr*gamma after each test')\n",
    "\n",
    "args = parser.parse_args(args=[])#(args=['--batch_size', '8',  '--no_cuda'])#used in ipynb\n",
    "print(args)\n",
    "\n",
    "dataset = load_dataset('glue', 'mnli')\n",
    "\n",
    "print('\\n Property of dataset:')\n",
    "print('train set size: ',len(dataset['train']))\n",
    "print('validation_matched set size: ',len(dataset['validation_matched']))\n",
    "print('validation_mismatched set size: ',len(dataset['validation_mismatched']))\n",
    "print('test_matched set size: ',len(dataset['test_matched']))\n",
    "print('test_mismatched set size: ',len(dataset['test_mismatched']))\n",
    "# %%\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "train = dataset['train'][:args.train_num_points]\n",
    "valid = dataset['validation_matched'][-args.valid_num_points:]\n",
    "replaced = replaced_data(valid, args.replace_size) \n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(args.model_name)\n",
    "#mnli\n",
    "#The Multi-Genre Natural Language Inference Corpus is a crowdsourced collection of sentence pairs with textual entailment annotations. Given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment), contradicts the hypothesis (contradiction), or neither (neutral). The premise sentences are gathered from ten different sources, including transcribed speech, fiction, and government reports. The authors of the benchmark use the standard test set, for which they obtained private labels from the RTE authors, and evaluate on both the matched (in-domain) and mismatched (cross-domain) section. They also uses and recommend the SNLI corpus as 550k examples of auxiliary training data.\n",
    "\n",
    "# %%\n",
    "train_data = get_Dataset(train, tokenizer,max_length=args.max_length)\n",
    "train_dataloader = DataLoader(train_data, sampler= SequentialSampler(train_data), \n",
    "                        batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)\n",
    "valid_data = get_Dataset(valid, tokenizer,max_length=args.max_length)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=SequentialSampler(valid_data), \n",
    "                        batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)\n",
    "replaced_data = get_Replaced_Dataset(replaced, tokenizer, max_length = args.max_length)\n",
    "replaced_dataloader = DataLoader(replaced_data, sampler=SequentialSampler(replaced_data), \n",
    "                        batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)\n",
    "\n",
    "# %%\n",
    "\n",
    "model = TextClassifier(args).to(device)\n",
    "model.train(train_dataloader,valid_dataloader,replaced_dataloader,device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_label_sensitivity(replaced_dataloader, original_dataloader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging    # first of all import the module\n",
    "from datetime import datetime\n",
    "\n",
    "logfilename = datetime.now().strftime('./logs/logfile_%H_%M_%d_%m_%Y.log')\n",
    "\n",
    "logging.basicConfig(filename=logfilename, filemode='w', format='%(name)s - %(levelname)s - %(message)s')\n",
    "logging.warning(\"warning\")\n",
    "logging.info(\"info\")\n",
    "# %%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
